---
applyTo: "{data,etl,ingestion,pipelines,adf,synapse,fabric,databricks}/**"
---

# Instructions Data Integration (Azure)

Quand tu travailles sur lâ€™intÃ©gration de donnÃ©es, appliquer ces rÃ¨gles :

## ğŸ¯ Objectifs
- Pipelines **robustes**, **observables** et **idempotents**
- Charges **incrÃ©mentales** privilÃ©giÃ©es
- **ParamÃ©trage** systÃ©matique (pas de valeurs en dur)

## ğŸ§± Stack Azure Prioritaire
- **Orchestration** : Azure Data Factory / Synapse Pipelines / Fabric Data Factory
- **Streaming** : Event Hubs, Stream Analytics
- **Stockage** : ADLS Gen2 / Blob Storage
- **Traitement** : Databricks / Synapse Spark / Fabric Lakehouse
- **Gouvernance** : Microsoft Purview
- **SÃ©curitÃ©** : Managed Identity + Key Vault

## âœ… Bonnes pratiques
- **Naming** clair (source â†’ cible, domaine, frÃ©quence)
- **Linked Services** avec Managed Identity
- **Retries** + **backoff** configurÃ©s
- **Checksum** ou watermark pour lâ€™incrÃ©mental
- **Schema drift** gÃ©rÃ© explicitement
- **Data quality checks** (nulls, types, volumes, doublons)
- **Alerting** sur failures et SLA dÃ©passÃ©es

## ğŸ“Œ Exemples de conventions

### Dataset naming
- `ds_src_sales_orders`
- `ds_curated_customer_dim`

### Pipeline naming
- `pl_ingest_sales_daily`
- `pl_curate_customer_dim`

### ParamÃ¨tres standards
- `p_run_id`, `p_watermark`, `p_source`, `p_sink`

## ğŸ” ObservabilitÃ©
- Loguer : `run_id`, `source`, `rows_read`, `rows_written`, `duration`
- GÃ©nÃ©rer un rapport dâ€™exÃ©cution en sortie

## ğŸ” SÃ©curitÃ©
- Secrets dans **Key Vault**
- IdentitÃ©s managÃ©es (pas de credentials locaux)
- AccÃ¨s storage via RBAC + ACLs ADLS
